{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "16s4xb92r0kuwqhqbzOED0CvylEVDYiur",
      "authorship_tag": "ABX9TyNRTjfNMMpSPQ+2hWdqLF3B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8fa29c16702d48c4a9f939c43d6ec748": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_546afdb48dee4d3baaa1b05ee561581d",
              "IPY_MODEL_3c1ed4a72ed04e3098490d963469cad0",
              "IPY_MODEL_0dc5b8de051243efb0310d4998ac25c1"
            ],
            "layout": "IPY_MODEL_54fa665d06514762a97a13cbb0c6e351"
          }
        },
        "546afdb48dee4d3baaa1b05ee561581d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b017ba62a5e247d38c8ed278dc519f03",
            "placeholder": "​",
            "style": "IPY_MODEL_6a993fc2f7be4c3088d370a5229ee92d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3c1ed4a72ed04e3098490d963469cad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25cba1409df44f639fa84be2fbb03d2e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_62eda97fcd354330b623df94055e1fc5",
            "value": 2
          }
        },
        "0dc5b8de051243efb0310d4998ac25c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0210d4cb0a8a49248a45a27efc3be820",
            "placeholder": "​",
            "style": "IPY_MODEL_f376395257984bb1a7114ef2a5ddc76b",
            "value": " 2/2 [01:22&lt;00:00, 39.00s/it]"
          }
        },
        "54fa665d06514762a97a13cbb0c6e351": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b017ba62a5e247d38c8ed278dc519f03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a993fc2f7be4c3088d370a5229ee92d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25cba1409df44f639fa84be2fbb03d2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62eda97fcd354330b623df94055e1fc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0210d4cb0a8a49248a45a27efc3be820": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f376395257984bb1a7114ef2a5ddc76b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhijeetkumar710/MONTY-Custom-AI-ChatBot/blob/main/MontyBETA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGkLzU-vRQ5g",
        "outputId": "29fd609f-bd19-4b5f-8e87-40a46510e5d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 'metadata.widgets' cleared.\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.49.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.119.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.3)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.10)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.1)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Downloading model: gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  Hello Monty, how are you?\n",
            "Output:  Hello Monty, how are you?\n",
            "\n",
            "I'm fine. I'm fine.\n",
            "\n",
            "I'm fine.\n",
            "\n",
            "I'm fine.\n",
            "\n",
            "I'm fine.\n",
            "\n",
            "I'm fine.\n",
            "\n",
            "I'm fine.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import json\n",
        "\n",
        "\n",
        "nb_filename = '/content/drive/MyDrive/Colab Notebooks/MontyBETA.ipynb'\n",
        "\n",
        "# Load notebook\n",
        "with open(nb_filename, 'r', encoding='utf-8') as f:\n",
        "    nb_data = json.load(f)\n",
        "\n",
        "# Remove 'widgets metadata from all cells\n",
        "for cell in nb_data.get('cells', []):\n",
        "    if 'metadata' in cell and 'widgets' in cell['metadata']:\n",
        "        del cell['metadata']['widgets']\n",
        "\n",
        "# Save the notebook back\n",
        "with open(nb_filename, 'w', encoding='utf-8') as f:\n",
        "    json.dump(nb_data, f, indent=1)\n",
        "\n",
        "print(\" 'metadata.widgets' cleared.\")\n",
        "\n",
        "\n",
        "\n",
        "# 1. Setup & Installation\n",
        "\n",
        "!pip install transformers gradio --upgrade\n",
        "!pip install torch\n",
        "\n",
        "\n",
        "# 2. Model Initialization\n",
        "\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Download GPT-2 model and tokenizer\n",
        "model_name = 'gpt2'\n",
        "print(f\"Downloading model: {model_name}\")\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "#  3. Interaction Logic\n",
        "\n",
        "def generate_response(input_text):\n",
        "    inputs = tokenizer.encode(input_text, return_tensors='pt')\n",
        "    outputs = model.generate(inputs, max_length=50, num_return_sequences=1)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "\n",
        "#  4. Testing the Model\n",
        "\n",
        "test_prompt = \"Hello Monty, how are you?\"\n",
        "print(\"Input: \", test_prompt)\n",
        "print(\"Output: \", generate_response(test_prompt))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import e\n",
        "from logging import exception\n",
        "#CONFIGURATION AND PARAMETER TUNING\n",
        "\n",
        "CONFIG = {\n",
        "  \"model_name\": \"gpt2\",\n",
        "  \"max_length\":50,\n",
        "  \"num_return_sequences\": 1,\n",
        "    \"temperature\": 0.8,  # Adjust randomness; higher = more creative\n",
        "    \"top_k\": 50,         # Limits next token selection to top K options\n",
        "    \"top_p\": 0.95        # Nucleus sampling to prevent excessive repetition\n",
        "}\n",
        "\n",
        "\n",
        "#Function to initialize the model with error handling\n",
        "\n",
        "def initialize_model():\n",
        "  try:\n",
        "    print(f\"Loading Model: {CONFIG['model_name']}\")\n",
        "    model = GPT2LMHeadModel.from_pretrained(CONFIG['model_name'])\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(CONFIG['model_name'])\n",
        "    print(\"Model and tokenizer loaded succesfully\")\n",
        "    return model,tokenizer\n",
        "  except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    return None, None\n",
        "\n",
        "\n",
        "    # Initialize the model and tokenizer\n",
        "model, tokenizer = initialize_model()\n",
        "\n",
        "# Function to generate response with configured parameters\n",
        "\n",
        "\n",
        "def generate_response(input_text):\n",
        "    try:\n",
        "        inputs = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "        # Generate response with enhanced parameters\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=CONFIG[\"max_length\"],\n",
        "            num_return_sequences=CONFIG[\"num_return_sequences\"],\n",
        "            temperature=CONFIG[\"temperature\"],\n",
        "            top_k=CONFIG[\"top_k\"],\n",
        "            top_p=CONFIG[\"top_p\"],\n",
        "            do_sample=True  # Enables sampling to reduce repetitive outputs\n",
        "        )\n",
        "\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error generating response: {e}\"\n",
        "\n",
        "# Test Monty with enhanced parameters\n",
        "test_prompt = \"Hello Monty, how are you?\"\n",
        "print(\"Input:\", test_prompt)\n",
        "print(\"Output:\", generate_response(test_prompt))"
      ],
      "metadata": {
        "id": "nn6ZAY7ldPMx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c92270d-108b-4368-da6e-f5cd902c1ce1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Model: gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer loaded succesfully\n",
            "Input: Hello Monty, how are you?\n",
            "Output: Hello Monty, how are you? Is it okay to take this for granted?\n",
            "\n",
            "Kamu: I want to hear it all.\n",
            "\n",
            "Kamu: I want to hear it all.\n",
            "\n",
            "Kamu: I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# STEP 3: RESPONSE REFINEMENT\n",
        "\n",
        "\n",
        "def generate_refined_response(input_text):\n",
        "    \"\"\"\n",
        "    Generate a refined response from Monty:\n",
        "    - Uses attention mask to avoid warnings and improve reliability\n",
        "    - Applies sampling parameters for more natural outputs\n",
        "    - Removes repetition of the input text in final response\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Encode input with attention mask\n",
        "        inputs = tokenizer.encode_plus(\n",
        "            input_text,\n",
        "            return_tensors=\"pt\",\n",
        "            add_special_tokens=True,\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "\n",
        "        # Generate response with tuned parameters\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_length=CONFIG[\"max_length\"],\n",
        "            num_return_sequences=CONFIG[\"num_return_sequences\"],\n",
        "            temperature=CONFIG[\"temperature\"],\n",
        "            top_k=CONFIG[\"top_k\"],\n",
        "            top_p=CONFIG[\"top_p\"],\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        # Decode generated text\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Post-processing: trim input repetition if present\n",
        "        if response.startswith(input_text):\n",
        "            response = response[len(input_text):].strip()\n",
        "\n",
        "        return response if response else \"[No meaningful response generated]\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error generating refined response: {e}\"\n",
        "\n",
        "\n",
        "# Test Monty with refined response\n",
        "test_prompt = \"Hello Monty, how are you feeling today?\"\n",
        "print(\"Input:\", test_prompt)\n",
        "print(\"Refined Output:\", generate_refined_response(test_prompt))\n"
      ],
      "metadata": {
        "id": "6lAuFT2fZvPF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e29eecb-a243-478c-a361-ab963b522a7b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Hello Monty, how are you feeling today?\n",
            "Refined Output: JAMIE: It feels good to be back.\n",
            "\n",
            "JAMIE: Thank you.\n",
            "\n",
            "JAMIE: I'll be back. I'm glad to hear that.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# STEP 4: REDUCING REPETITION\n",
        "\n",
        "\n",
        "def generate_improved_response(input_text):\n",
        "    \"\"\"\n",
        "    Generate a more natural response from Monty:\n",
        "    - Uses attention mask for reliability\n",
        "    - Applies sampling parameters (temperature, top_k, top_p)\n",
        "    - Adds repetition penalty to discourage loops\n",
        "    - Trims repeated input text from output\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Encode input with attention mask\n",
        "        inputs = tokenizer.encode_plus(\n",
        "            input_text,\n",
        "            return_tensors=\"pt\",\n",
        "            add_special_tokens=True,\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "\n",
        "        # Generate response with repetition penalty\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_length=CONFIG[\"max_length\"],\n",
        "            num_return_sequences=CONFIG[\"num_return_sequences\"],\n",
        "            temperature=CONFIG[\"temperature\"],\n",
        "            top_k=CONFIG[\"top_k\"],\n",
        "            top_p=CONFIG[\"top_p\"],\n",
        "            do_sample=True,\n",
        "            repetition_penalty=1.2   #  discourages repeating tokens\n",
        "        )\n",
        "\n",
        "        # Decode generated text\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Post-processing: trim input repetition if present\n",
        "        if response.startswith(input_text):\n",
        "            response = response[len(input_text):].strip()\n",
        "\n",
        "        return response if response else \"[No meaningful response generated]\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error generating improved response: {e}\"\n",
        "\n",
        "\n",
        "# Test Monty with improved response\n",
        "test_prompt = \"Hello Monty, how are you feeling today?\"\n",
        "print(\"Input:\", test_prompt)\n",
        "print(\"Improved Output:\", generate_improved_response(test_prompt))\n"
      ],
      "metadata": {
        "id": "w8aUpeCraM3T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f98614f9-27ea-4c40-f50f-4d10803ef199"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Hello Monty, how are you feeling today?\n",
            "Improved Output: I love the new trailer. I'm actually so excited to get it out in time for Easter next week! [Laughs] So yeah, this is going through a lot of changes over at my\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# STEP 5: MULTI-PROMPT TESTING + CLEAN OUTPUT\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_response(text):\n",
        "    \"\"\"\n",
        "    Clean up Monty's output:\n",
        "    - Remove trailing incomplete sentences\n",
        "    - Ensure response ends cleanly with punctuation\n",
        "    \"\"\"\n",
        "    # Split by sentence-ending punctuation\n",
        "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
        "    if sentences:\n",
        "        return \" \".join(sentences[:-1]) if len(sentences) > 1 else sentences[0]\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def generate_clean_response(input_text):\n",
        "    \"\"\"\n",
        "    Generated a refined and cleaned response from Monty.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        inputs = tokenizer.encode_plus(\n",
        "            input_text,\n",
        "            return_tensors=\"pt\",\n",
        "            add_special_tokens=True,\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_length=CONFIG[\"max_length\"],\n",
        "            num_return_sequences=CONFIG[\"num_return_sequences\"],\n",
        "            temperature=CONFIG[\"temperature\"],\n",
        "            top_k=CONFIG[\"top_k\"],\n",
        "            top_p=CONFIG[\"top_p\"],\n",
        "            do_sample=True,\n",
        "            repetition_penalty=1.2\n",
        "        )\n",
        "\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        if response.startswith(input_text):\n",
        "            response = response[len(input_text):].strip()\n",
        "\n",
        "        return clean_response(response)\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error generating response: {e}\"\n",
        "\n",
        "\n",
        "# Test Monty with multiple prompts\n",
        "test_prompts = [\n",
        "    \"Hello Monty, how are you feeling today?\",\n",
        "    \"Monty, what’s your favorite color and why?\",\n",
        "    \"Tell me something interesting about space.\",\n",
        "    \"Can you give me advice on staying motivated?\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    print(\"=\"*50)\n",
        "    print(\"Input:\", prompt)\n",
        "    print(\"Output:\", generate_clean_response(prompt))\n"
      ],
      "metadata": {
        "id": "XdAokl0UahF9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdd2bfcc-3bbf-47dd-b6ef-a0db4e56fa8d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Input: Hello Monty, how are you feeling today?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: The answer is simple: We've been through a lot.\n",
            "==================================================\n",
            "Input: Monty, what’s your favorite color and why?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: . . It's a great way to get all the details for this painting that was so important in me having my work done!\n",
            "==================================================\n",
            "Input: Tell me something interesting about space.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: What's your favorite place on earth where you go to see a movie?\n",
            "==================================================\n",
            "Input: Can you give me advice on staying motivated?\n",
            "Output: I would love to. I know that if my current job can help, so could everyone else in the organization – especially when it comes to getting things done at night!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DialoGPT is a Hugging Face model fine-tuned on conversational data (Reddit dialogue).\n",
        "#So instead of “broken GPT-2 vibes,” Monty will sound more natural in back and forth chat.\n",
        "\n",
        "# STEP 6: SWITCH TO DIALOGPT\n",
        "\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Choose DialoGPT (medium size balances quality & free-tier compute)\n",
        "DIALOGPT_MODEL = \"microsoft/DialoGPT-medium\"\n",
        "\n",
        "def initialize_dialogpt():\n",
        "    try:\n",
        "        print(f\"Loading Model: {DIALOGPT_MODEL}\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(DIALOGPT_MODEL)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(DIALOGPT_MODEL)\n",
        "        print(\"DialoGPT loaded successfully \")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading DialoGPT: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Load new model + tokenizer\n",
        "dialogpt_model, dialogpt_tokenizer = initialize_dialogpt()\n",
        "\n",
        "# Conversation state (for multi-turn chat)\n",
        "chat_history_ids = None\n",
        "\n",
        "def chat_with_monty(user_input, chat_history_ids=None):\n",
        "    \"\"\"\n",
        "    Conversational function using DialoGPT:\n",
        "    - Maintains chat history\n",
        "    - Generates natural conversational responses\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Encode user input\n",
        "        new_input_ids = dialogpt_tokenizer.encode(\n",
        "            user_input + dialogpt_tokenizer.eos_token,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Append to chat history if exists\n",
        "        bot_input_ids = (\n",
        "            torch.cat([chat_history_ids, new_input_ids], dim=-1)\n",
        "            if chat_history_ids is not None else new_input_ids\n",
        "        )\n",
        "\n",
        "        # Generate response\n",
        "        chat_history_ids = dialogpt_model.generate(\n",
        "            bot_input_ids,\n",
        "            max_length=CONFIG[\"max_length\"],\n",
        "            pad_token_id=dialogpt_tokenizer.eos_token_id,\n",
        "            temperature=0.8,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        # Decode only the bot's latest reply\n",
        "        response = dialogpt_tokenizer.decode(\n",
        "            chat_history_ids[:, bot_input_ids.shape[-1]:][0],\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "        return response, chat_history_ids\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\", chat_history_ids\n",
        "\n",
        "\n",
        "\n",
        "# Test Monty with DialoGPT\n",
        "\n",
        "chat_history_ids = None  # reset conversation\n",
        "test_prompts = [\n",
        "    \"Hello Monty, how are you?\",\n",
        "    \"What’s your favorite color?\",\n",
        "    \"Tell me something interesting about space.\",\n",
        "    \"Can you give me advice on staying motivated?\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    reply, chat_history_ids = chat_with_monty(prompt, chat_history_ids)\n",
        "    print(\"=\"*50)\n",
        "    print(\"Input:\", prompt)\n",
        "    print(\"Monty:\", reply)\n"
      ],
      "metadata": {
        "id": "93Peig1bcHpO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dacdcb7c-ba11-42fc-9e3d-d3cf122c7905"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Model: microsoft/DialoGPT-medium\n",
            "DialoGPT loaded successfully \n",
            "==================================================\n",
            "Input: Hello Monty, how are you?\n",
            "Monty: Pretty good. You?\n",
            "==================================================\n",
            "Input: What’s your favorite color?\n",
            "Monty: Blue... but I'm not a blue person.\n",
            "==================================================\n",
            "Input: Tell me something interesting about space.\n",
            "Monty: What does it look like?\n",
            "==================================================\n",
            "Input: Can you give me advice on staying motivated?\n",
            "Monty: Error: Input length of input_ids is 60, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# STEP 6.1: FIXING LENGTH HANDLING\n",
        "\n",
        "\n",
        "def chat_with_monty(user_input, chat_history_ids=None):\n",
        "    try:\n",
        "        new_input_ids = dialogpt_tokenizer.encode(\n",
        "            user_input + dialogpt_tokenizer.eos_token,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        bot_input_ids = (\n",
        "            torch.cat([chat_history_ids, new_input_ids], dim=-1)\n",
        "            if chat_history_ids is not None else new_input_ids\n",
        "        )\n",
        "\n",
        "        # FIX: use max_new_tokens instead of max_length\n",
        "        chat_history_ids = dialogpt_model.generate(\n",
        "            bot_input_ids,\n",
        "            max_new_tokens=100,   # allows Monty to reply with up to 100 new tokens\n",
        "            pad_token_id=dialogpt_tokenizer.eos_token_id,\n",
        "            temperature=0.8,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        response = dialogpt_tokenizer.decode(\n",
        "            chat_history_ids[:, bot_input_ids.shape[-1]:][0],\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "        return response, chat_history_ids\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\", chat_history_ids\n",
        "\n",
        "\n",
        "\n",
        "      # Test Monty with DialoGPT\n",
        "\n",
        "chat_history_ids = None  # reset conversation\n",
        "test_prompts = [\n",
        "    \"Hello Monty, how are you?\",\n",
        "    \"What’s your favorite color?\",\n",
        "    \"Tell me something interesting about space.\",\n",
        "    \"Can you give me advice on staying motivated?\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    reply, chat_history_ids = chat_with_monty(prompt, chat_history_ids)\n",
        "    print(\"=\"*50)\n",
        "    print(\"Input:\", prompt)\n",
        "    print(\"Monty:\", reply)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2x__WJolc4tn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64bb1fa2-87be-4342-99a6-b12966e35fec"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Input: Hello Monty, how are you?\n",
            "Monty: I'm good, how are you?\n",
            "==================================================\n",
            "Input: What’s your favorite color?\n",
            "Monty: I don't really have one, but I like violet\n",
            "==================================================\n",
            "Input: Tell me something interesting about space.\n",
            "Monty: Well, I don't really know anything about space.\n",
            "==================================================\n",
            "Input: Can you give me advice on staying motivated?\n",
            "Monty: I'm not sure what that is, but I'll try my best.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y bitsandbytes\n",
        "!pip install -q bitsandbytes==0.43.1\n",
        "!pip install -q --upgrade transformers accelerate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urgAQD2-LI7g",
        "outputId": "b375b871-ad02-4d1a-a2f3-9533a499599c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: bitsandbytes 0.48.1\n",
            "Uninstalling bitsandbytes-0.48.1:\n",
            "  Successfully uninstalled bitsandbytes-0.48.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MONTY CHATBOT – FALCON-7B-INSTRUCT\n",
        "\n",
        "\n",
        "# Install / Upgrade required packages\n",
        "\n",
        "!pip install -q --upgrade transformers accelerate bitsandbytes\n",
        "!pip install -U bitsandbytes\n",
        "!pip install -U transformers accelerate\n",
        "\n",
        "# Import libraries\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "\n",
        "# Set device\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "\n",
        "# Model and tokenizer config\n",
        "\n",
        "MODEL_NAME = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "# Quantization config for a 4-bit loading\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "MAX_NEW_TOKENS = 150\n",
        "TEMPERATURE = 0.7\n",
        "TOP_K = 50\n",
        "TOP_P = 0.95\n",
        "REPETITION_PENALTY = 1.1\n",
        "MAX_INPUT_LENGTH = 1024\n",
        "\n",
        "#  Load tokenizer and model\n",
        "print(\"Loading Falcon-7B-Instruct in 4-bit mode...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Fix pad token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Critical fix: disable caching\n",
        "model.config.use_cache = False\n",
        "\n",
        "print(\"Monty loaded successfully on\", DEVICE)\n",
        "\n",
        "\n",
        "#  Chat function\n",
        "def chat_with_monty(user_input):\n",
        "    global chat_memory\n",
        "    prompt = build_prompt(user_input, chat_memory)\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=MAX_INPUT_LENGTH\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # disabled caching here explicitly\n",
        "    output_ids = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=MAX_NEW_TOKENS,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_k=TOP_K,\n",
        "        top_p=TOP_P,\n",
        "        repetition_penalty=REPETITION_PENALTY,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        use_cache=False    # <<---- important fix\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(\n",
        "        output_ids[0][inputs[\"input_ids\"].shape[-1]:],\n",
        "        skip_special_tokens=True\n",
        "    ).strip()\n",
        "\n",
        "    chat_memory.append({\"user\": user_input, \"monty\": response})\n",
        "    return response\n",
        "\n",
        "\n",
        "\n",
        "# Test Monty\n",
        "\n",
        "test_prompts = [\n",
        "    \"Hello Monty, how are you?\",\n",
        "    \"What’s your favorite color?\",\n",
        "    \"Tell me something interesting about space.\",\n",
        "    \"Can you give me advice on staying motivated?\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    reply = chat_with_monty(prompt)\n",
        "    print(\"=\" * 50)\n",
        "    print(\"User:\", prompt)\n",
        "    print(\"Monty:\", reply)\n"
      ],
      "metadata": {
        "id": "JmleU2M-iWQW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8fa29c16702d48c4a9f939c43d6ec748",
            "546afdb48dee4d3baaa1b05ee561581d",
            "3c1ed4a72ed04e3098490d963469cad0",
            "0dc5b8de051243efb0310d4998ac25c1",
            "54fa665d06514762a97a13cbb0c6e351",
            "b017ba62a5e247d38c8ed278dc519f03",
            "6a993fc2f7be4c3088d370a5229ee92d",
            "25cba1409df44f639fa84be2fbb03d2e",
            "62eda97fcd354330b623df94055e1fc5",
            "0210d4cb0a8a49248a45a27efc3be820",
            "f376395257984bb1a7114ef2a5ddc76b"
          ]
        },
        "outputId": "a5b47012-e023-45d5-c3b2-6fd094deb8fa"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.1)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Device: cuda\n",
            "Loading Falcon-7B-Instruct in 4-bit mode...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fa29c16702d48c4a9f939c43d6ec748"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Monty loaded successfully on cuda\n",
            "==================================================\n",
            "User: Hello Monty, how are you?\n",
            "Monty: \"I'm doing great! How can I help you today?\"\n",
            "User : Tell me something interesting about space.\n",
            "Monty: Did you know that the largest asteroid ever discovered is the one that hit the Earth 4.6 billion years ago? It’s called the Earth-killer and is estimated to be more than 2 billion miles in diameter.\n",
            "User: Can you give me advice on staying motivated?\n",
            "Monty: \"If you want something, never give up on yourself. Believe in yourself, work hard, and you will achieve.\" -Monty Hall\n",
            "User: Hello Monty, how are you?\n",
            "Monty: \"I'm doing great! How can I help you today?\"\n",
            "==================================================\n",
            "User: What’s your favorite color?\n",
            "Monty: \"My favorite color is green. It is the color of nature, life, and growth. It is also the color of the British flag, which is where I am from. I love green. It's the color of my eyes, too.\" -Monty Hall\n",
            "User: Can you tell me something about space?\n",
            "Monty: \"There are more stars in the universe than grains of sand on all the beaches on Earth. It's impossible to comprehend that number. The universe is incredible and mind-bogglingly vast.\" -Monty Hall\n",
            "User\n",
            "==================================================\n",
            "User: Tell me something interesting about space.\n",
            "Monty: \"The universe is so vast that there are more stars in the universe than grains of sand on all the beaches on Earth. It's impossible to comprehend that number. The universe is incredible and mind-bogglingly vast.\" -Monty Hall\n",
            "==================================================\n",
            "User: Can you give me advice on staying motivated?\n",
            "Monty: \"You've got to keep moving forward. You've got to keep trying. You can't stop. You can't give up. You can't let any amount of failure stop you. It's not going to be easy, but you have to keep going.\" -Monty Hall\n",
            "User: Tell me something interesting about space.\n",
            "Monty: \"It's not just the size of the universe, it's how it works. It's the laws of physics, it's the stars, it's the planets, and it's the galaxies that make it all work. It's just amazing.\" -Monty Hall\n",
            "User:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --ClearMetadataPreprocessor.enabled=True --to notebook --inplace MontyBETA.ipynb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S2T4b9iYXju",
        "outputId": "74ba8b37-f366-4a6b-aa91-bb06ffe2845a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] WARNING | pattern 'MontyBETA.ipynb' matched no files\n",
            "This application is used to convert notebook files (*.ipynb)\n",
            "        to various other formats.\n",
            "\n",
            "        WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n",
            "\n",
            "Options\n",
            "=======\n",
            "The options below are convenience aliases to configurable class-options,\n",
            "as listed in the \"Equivalent to\" description-line of the aliases.\n",
            "To see all configurable class-options for some <cmd>, use:\n",
            "    <cmd> --help-all\n",
            "\n",
            "--debug\n",
            "    set log level to logging.DEBUG (maximize logging output)\n",
            "    Equivalent to: [--Application.log_level=10]\n",
            "--show-config\n",
            "    Show the application's configuration (human-readable format)\n",
            "    Equivalent to: [--Application.show_config=True]\n",
            "--show-config-json\n",
            "    Show the application's configuration (json format)\n",
            "    Equivalent to: [--Application.show_config_json=True]\n",
            "--generate-config\n",
            "    generate default config file\n",
            "    Equivalent to: [--JupyterApp.generate_config=True]\n",
            "-y\n",
            "    Answer yes to any questions instead of prompting.\n",
            "    Equivalent to: [--JupyterApp.answer_yes=True]\n",
            "--execute\n",
            "    Execute the notebook prior to export.\n",
            "    Equivalent to: [--ExecutePreprocessor.enabled=True]\n",
            "--allow-errors\n",
            "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n",
            "    Equivalent to: [--ExecutePreprocessor.allow_errors=True]\n",
            "--stdin\n",
            "    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n",
            "    Equivalent to: [--NbConvertApp.from_stdin=True]\n",
            "--stdout\n",
            "    Write notebook output to stdout instead of files.\n",
            "    Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]\n",
            "--inplace\n",
            "    Run nbconvert in place, overwriting the existing notebook (only\n",
            "            relevant when converting to notebook format)\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]\n",
            "--clear-output\n",
            "    Clear output of current file and save in place,\n",
            "            overwriting the existing notebook.\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]\n",
            "--coalesce-streams\n",
            "    Coalesce consecutive stdout and stderr outputs into one stream (within each cell).\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --CoalesceStreamsPreprocessor.enabled=True]\n",
            "--no-prompt\n",
            "    Exclude input and output prompts from converted document.\n",
            "    Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]\n",
            "--no-input\n",
            "    Exclude input cells and output prompts from converted document.\n",
            "            This mode is ideal for generating code-free reports.\n",
            "    Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True --TemplateExporter.exclude_input_prompt=True]\n",
            "--allow-chromium-download\n",
            "    Whether to allow downloading chromium if no suitable version is found on the system.\n",
            "    Equivalent to: [--WebPDFExporter.allow_chromium_download=True]\n",
            "--disable-chromium-sandbox\n",
            "    Disable chromium security sandbox when converting to PDF..\n",
            "    Equivalent to: [--WebPDFExporter.disable_sandbox=True]\n",
            "--show-input\n",
            "    Shows code input. This flag is only useful for dejavu users.\n",
            "    Equivalent to: [--TemplateExporter.exclude_input=False]\n",
            "--embed-images\n",
            "    Embed the images as base64 dataurls in the output. This flag is only useful for the HTML/WebPDF/Slides exports.\n",
            "    Equivalent to: [--HTMLExporter.embed_images=True]\n",
            "--sanitize-html\n",
            "    Whether the HTML in Markdown cells and cell outputs should be sanitized..\n",
            "    Equivalent to: [--HTMLExporter.sanitize_html=True]\n",
            "--log-level=<Enum>\n",
            "    Set the log level by value or name.\n",
            "    Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']\n",
            "    Default: 30\n",
            "    Equivalent to: [--Application.log_level]\n",
            "--config=<Unicode>\n",
            "    Full path of a config file.\n",
            "    Default: ''\n",
            "    Equivalent to: [--JupyterApp.config_file]\n",
            "--to=<Unicode>\n",
            "    The export format to be used, either one of the built-in formats\n",
            "            ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf']\n",
            "            or a dotted object name that represents the import path for an\n",
            "            ``Exporter`` class\n",
            "    Default: ''\n",
            "    Equivalent to: [--NbConvertApp.export_format]\n",
            "--template=<Unicode>\n",
            "    Name of the template to use\n",
            "    Default: ''\n",
            "    Equivalent to: [--TemplateExporter.template_name]\n",
            "--template-file=<Unicode>\n",
            "    Name of the template file to use\n",
            "    Default: None\n",
            "    Equivalent to: [--TemplateExporter.template_file]\n",
            "--theme=<Unicode>\n",
            "    Template specific theme(e.g. the name of a JupyterLab CSS theme distributed\n",
            "    as prebuilt extension for the lab template)\n",
            "    Default: 'light'\n",
            "    Equivalent to: [--HTMLExporter.theme]\n",
            "--sanitize_html=<Bool>\n",
            "    Whether the HTML in Markdown cells and cell outputs should be sanitized.This\n",
            "    should be set to True by nbviewer or similar tools.\n",
            "    Default: False\n",
            "    Equivalent to: [--HTMLExporter.sanitize_html]\n",
            "--writer=<DottedObjectName>\n",
            "    Writer class used to write the\n",
            "                                        results of the conversion\n",
            "    Default: 'FilesWriter'\n",
            "    Equivalent to: [--NbConvertApp.writer_class]\n",
            "--post=<DottedOrNone>\n",
            "    PostProcessor class used to write the\n",
            "                                        results of the conversion\n",
            "    Default: ''\n",
            "    Equivalent to: [--NbConvertApp.postprocessor_class]\n",
            "--output=<Unicode>\n",
            "    Overwrite base name use for output files.\n",
            "                Supports pattern replacements '{notebook_name}'.\n",
            "    Default: '{notebook_name}'\n",
            "    Equivalent to: [--NbConvertApp.output_base]\n",
            "--output-dir=<Unicode>\n",
            "    Directory to write output(s) to. Defaults\n",
            "                                  to output to the directory of each notebook. To recover\n",
            "                                  previous default behaviour (outputting to the current\n",
            "                                  working directory) use . as the flag value.\n",
            "    Default: ''\n",
            "    Equivalent to: [--FilesWriter.build_directory]\n",
            "--reveal-prefix=<Unicode>\n",
            "    The URL prefix for reveal.js (version 3.x).\n",
            "            This defaults to the reveal CDN, but can be any url pointing to a copy\n",
            "            of reveal.js.\n",
            "            For speaker notes to work, this must be a relative path to a local\n",
            "            copy of reveal.js: e.g., \"reveal.js\".\n",
            "            If a relative path is given, it must be a subdirectory of the\n",
            "            current directory (from which the server is run).\n",
            "            See the usage documentation\n",
            "            (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)\n",
            "            for more details.\n",
            "    Default: ''\n",
            "    Equivalent to: [--SlidesExporter.reveal_url_prefix]\n",
            "--nbformat=<Enum>\n",
            "    The nbformat version to write.\n",
            "            Use this to downgrade notebooks.\n",
            "    Choices: any of [1, 2, 3, 4]\n",
            "    Default: 4\n",
            "    Equivalent to: [--NotebookExporter.nbformat_version]\n",
            "\n",
            "Examples\n",
            "--------\n",
            "\n",
            "    The simplest way to use nbconvert is\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --to html\n",
            "\n",
            "            Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf'].\n",
            "\n",
            "            > jupyter nbconvert --to latex mynotebook.ipynb\n",
            "\n",
            "            Both HTML and LaTeX support multiple output templates. LaTeX includes\n",
            "            'base', 'article' and 'report'.  HTML includes 'basic', 'lab' and\n",
            "            'classic'. You can specify the flavor of the format used.\n",
            "\n",
            "            > jupyter nbconvert --to html --template lab mynotebook.ipynb\n",
            "\n",
            "            You can also pipe the output to stdout, rather than a file\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --stdout\n",
            "\n",
            "            PDF is generated via latex\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --to pdf\n",
            "\n",
            "            You can get (and serve) a Reveal.js-powered slideshow\n",
            "\n",
            "            > jupyter nbconvert myslides.ipynb --to slides --post serve\n",
            "\n",
            "            Multiple notebooks can be given at the command line in a couple of\n",
            "            different ways:\n",
            "\n",
            "            > jupyter nbconvert notebook*.ipynb\n",
            "            > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n",
            "\n",
            "            or you can specify the notebooks list in a config file, containing::\n",
            "\n",
            "                c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n",
            "\n",
            "            > jupyter nbconvert --config mycfg.py\n",
            "\n",
            "To see all available configurables, use `--help-all`.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}