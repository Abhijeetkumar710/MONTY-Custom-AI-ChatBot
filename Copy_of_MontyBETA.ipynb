{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "16s4xb92r0kuwqhqbzOED0CvylEVDYiur",
      "authorship_tag": "ABX9TyMPps7vfDk8w3fhe3zvTZGq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhijeetkumar710/MONTY-Custom-AI-ChatBot/blob/main/Copy_of_MontyBETA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGkLzU-vRQ5g"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import json\n",
        "\n",
        "\n",
        "nb_filename = '/content/drive/MyDrive/Colab Notebooks/MontyBETA.ipynb'\n",
        "\n",
        "# Load notebook\n",
        "with open(nb_filename, 'r', encoding='utf-8') as f:\n",
        "    nb_data = json.load(f)\n",
        "\n",
        "# Remove 'widgets metadata from all cells\n",
        "for cell in nb_data.get('cells', []):\n",
        "    if 'metadata' in cell and 'widgets' in cell['metadata']:\n",
        "        del cell['metadata']['widgets']\n",
        "\n",
        "# Save the notebook back\n",
        "with open(nb_filename, 'w', encoding='utf-8') as f:\n",
        "    json.dump(nb_data, f, indent=1)\n",
        "\n",
        "print(\" 'metadata.widgets' cleared.\")\n",
        "\n",
        "\n",
        "\n",
        "# 1. Setup & Installation\n",
        "\n",
        "!pip install transformers gradio --upgrade\n",
        "!pip install torch\n",
        "\n",
        "\n",
        "# 2. Model Initialization\n",
        "\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Download GPT-2 model and tokenizer\n",
        "model_name = 'gpt2'\n",
        "print(f\"Downloading model: {model_name}\")\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "#  3. Interaction Logic\n",
        "\n",
        "def generate_response(input_text):\n",
        "    inputs = tokenizer.encode(input_text, return_tensors='pt')\n",
        "    outputs = model.generate(inputs, max_length=50, num_return_sequences=1)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "\n",
        "#  4. Testing the Model\n",
        "\n",
        "test_prompt = \"Hello Monty, how are you?\"\n",
        "print(\"Input: \", test_prompt)\n",
        "print(\"Output: \", generate_response(test_prompt))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import e\n",
        "from logging import exception\n",
        "#CONFIGURATION AND PARAMETER TUNING\n",
        "\n",
        "CONFIG = {\n",
        "  \"model_name\": \"gpt2\",\n",
        "  \"max_length\":50,\n",
        "  \"num_return_sequences\": 1,\n",
        "    \"temperature\": 0.8,  # Adjust randomness; higher = more creative\n",
        "    \"top_k\": 50,         # Limits next token selection to top K options\n",
        "    \"top_p\": 0.95        # Nucleus sampling to prevent excessive repetition\n",
        "}\n",
        "\n",
        "\n",
        "#Function to initialize the model with error handling\n",
        "\n",
        "def initialize_model():\n",
        "  try:\n",
        "    print(f\"Loading Model: {CONFIG['model_name']}\")\n",
        "    model = GPT2LMHeadModel.from_pretrained(CONFIG['model_name'])\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(CONFIG['model_name'])\n",
        "    print(\"Model and tokenizer loaded succesfully\")\n",
        "    return model,tokenizer\n",
        "  except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    return None, None\n",
        "\n",
        "\n",
        "    # Initialize the model and tokenizer\n",
        "model, tokenizer = initialize_model()\n",
        "\n",
        "# Function to generate response with configured parameters\n",
        "\n",
        "\n",
        "def generate_response(input_text):\n",
        "    try:\n",
        "        inputs = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "        # Generate response with enhanced parameters\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=CONFIG[\"max_length\"],\n",
        "            num_return_sequences=CONFIG[\"num_return_sequences\"],\n",
        "            temperature=CONFIG[\"temperature\"],\n",
        "            top_k=CONFIG[\"top_k\"],\n",
        "            top_p=CONFIG[\"top_p\"],\n",
        "            do_sample=True  # Enables sampling to reduce repetitive outputs\n",
        "        )\n",
        "\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error generating response: {e}\"\n",
        "\n",
        "# Test Monty with enhanced parameters\n",
        "test_prompt = \"Hello Monty, how are you?\"\n",
        "print(\"Input:\", test_prompt)\n",
        "print(\"Output:\", generate_response(test_prompt))"
      ],
      "metadata": {
        "id": "nn6ZAY7ldPMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# STEP 3: RESPONSE REFINEMENT\n",
        "\n",
        "\n",
        "def generate_refined_response(input_text):\n",
        "    \"\"\"\n",
        "    Generate a refined response from Monty:\n",
        "    - Uses attention mask to avoid warnings and improve reliability\n",
        "    - Applies sampling parameters for more natural outputs\n",
        "    - Removes repetition of the input text in final response\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Encode input with attention mask\n",
        "        inputs = tokenizer.encode_plus(\n",
        "            input_text,\n",
        "            return_tensors=\"pt\",\n",
        "            add_special_tokens=True,\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "\n",
        "        # Generate response with tuned parameters\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_length=CONFIG[\"max_length\"],\n",
        "            num_return_sequences=CONFIG[\"num_return_sequences\"],\n",
        "            temperature=CONFIG[\"temperature\"],\n",
        "            top_k=CONFIG[\"top_k\"],\n",
        "            top_p=CONFIG[\"top_p\"],\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        # Decode generated text\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Post-processing: trim input repetition if present\n",
        "        if response.startswith(input_text):\n",
        "            response = response[len(input_text):].strip()\n",
        "\n",
        "        return response if response else \"[No meaningful response generated]\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error generating refined response: {e}\"\n",
        "\n",
        "\n",
        "# Test Monty with refined response\n",
        "test_prompt = \"Hello Monty, how are you feeling today?\"\n",
        "print(\"Input:\", test_prompt)\n",
        "print(\"Refined Output:\", generate_refined_response(test_prompt))\n"
      ],
      "metadata": {
        "id": "6lAuFT2fZvPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# STEP 4: REDUCING REPETITION\n",
        "\n",
        "\n",
        "def generate_improved_response(input_text):\n",
        "    \"\"\"\n",
        "    Generate a more natural response from Monty:\n",
        "    - Uses attention mask for reliability\n",
        "    - Applies sampling parameters (temperature, top_k, top_p)\n",
        "    - Adds repetition penalty to discourage loops\n",
        "    - Trims repeated input text from output\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Encode input with attention mask\n",
        "        inputs = tokenizer.encode_plus(\n",
        "            input_text,\n",
        "            return_tensors=\"pt\",\n",
        "            add_special_tokens=True,\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "\n",
        "        # Generate response with repetition penalty\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_length=CONFIG[\"max_length\"],\n",
        "            num_return_sequences=CONFIG[\"num_return_sequences\"],\n",
        "            temperature=CONFIG[\"temperature\"],\n",
        "            top_k=CONFIG[\"top_k\"],\n",
        "            top_p=CONFIG[\"top_p\"],\n",
        "            do_sample=True,\n",
        "            repetition_penalty=1.2   #  discourages repeating tokens\n",
        "        )\n",
        "\n",
        "        # Decode generated text\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Post-processing: trim input repetition if present\n",
        "        if response.startswith(input_text):\n",
        "            response = response[len(input_text):].strip()\n",
        "\n",
        "        return response if response else \"[No meaningful response generated]\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error generating improved response: {e}\"\n",
        "\n",
        "\n",
        "# Test Monty with improved response\n",
        "test_prompt = \"Hello Monty, how are you feeling today?\"\n",
        "print(\"Input:\", test_prompt)\n",
        "print(\"Improved Output:\", generate_improved_response(test_prompt))\n"
      ],
      "metadata": {
        "id": "w8aUpeCraM3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# STEP 5: MULTI-PROMPT TESTING + CLEAN OUTPUT\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_response(text):\n",
        "    \"\"\"\n",
        "    Clean up Monty's output:\n",
        "    - Remove trailing incomplete sentences\n",
        "    - Ensure response ends cleanly with punctuation\n",
        "    \"\"\"\n",
        "    # Split by sentence-ending punctuation\n",
        "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
        "    if sentences:\n",
        "        return \" \".join(sentences[:-1]) if len(sentences) > 1 else sentences[0]\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def generate_clean_response(input_text):\n",
        "    \"\"\"\n",
        "    Generated a refined and cleaned response from Monty.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        inputs = tokenizer.encode_plus(\n",
        "            input_text,\n",
        "            return_tensors=\"pt\",\n",
        "            add_special_tokens=True,\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_length=CONFIG[\"max_length\"],\n",
        "            num_return_sequences=CONFIG[\"num_return_sequences\"],\n",
        "            temperature=CONFIG[\"temperature\"],\n",
        "            top_k=CONFIG[\"top_k\"],\n",
        "            top_p=CONFIG[\"top_p\"],\n",
        "            do_sample=True,\n",
        "            repetition_penalty=1.2\n",
        "        )\n",
        "\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        if response.startswith(input_text):\n",
        "            response = response[len(input_text):].strip()\n",
        "\n",
        "        return clean_response(response)\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error generating response: {e}\"\n",
        "\n",
        "\n",
        "# Test Monty with multiple prompts\n",
        "test_prompts = [\n",
        "    \"Hello Monty, how are you feeling today?\",\n",
        "    \"Monty, what’s your favorite color and why?\",\n",
        "    \"Tell me something interesting about space.\",\n",
        "    \"Can you give me advice on staying motivated?\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    print(\"=\"*50)\n",
        "    print(\"Input:\", prompt)\n",
        "    print(\"Output:\", generate_clean_response(prompt))\n"
      ],
      "metadata": {
        "id": "XdAokl0UahF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DialoGPT is a Hugging Face model fine-tuned on conversational data (Reddit dialogue).\n",
        "#So instead of “broken GPT-2 vibes,” Monty will sound more natural in back and forth chat.\n",
        "\n",
        "# STEP 6: SWITCH TO DIALOGPT\n",
        "\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Choose DialoGPT (medium size balances quality & free-tier compute)\n",
        "DIALOGPT_MODEL = \"microsoft/DialoGPT-medium\"\n",
        "\n",
        "def initialize_dialogpt():\n",
        "    try:\n",
        "        print(f\"Loading Model: {DIALOGPT_MODEL}\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(DIALOGPT_MODEL)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(DIALOGPT_MODEL)\n",
        "        print(\"DialoGPT loaded successfully \")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading DialoGPT: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Load new model + tokenizer\n",
        "dialogpt_model, dialogpt_tokenizer = initialize_dialogpt()\n",
        "\n",
        "# Conversation state (for multi-turn chat)\n",
        "chat_history_ids = None\n",
        "\n",
        "def chat_with_monty(user_input, chat_history_ids=None):\n",
        "    \"\"\"\n",
        "    Conversational function using DialoGPT:\n",
        "    - Maintains chat history\n",
        "    - Generates natural conversational responses\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Encode user input\n",
        "        new_input_ids = dialogpt_tokenizer.encode(\n",
        "            user_input + dialogpt_tokenizer.eos_token,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Append to chat history if exists\n",
        "        bot_input_ids = (\n",
        "            torch.cat([chat_history_ids, new_input_ids], dim=-1)\n",
        "            if chat_history_ids is not None else new_input_ids\n",
        "        )\n",
        "\n",
        "        # Generate response\n",
        "        chat_history_ids = dialogpt_model.generate(\n",
        "            bot_input_ids,\n",
        "            max_length=CONFIG[\"max_length\"],\n",
        "            pad_token_id=dialogpt_tokenizer.eos_token_id,\n",
        "            temperature=0.8,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        # Decode only the bot's latest reply\n",
        "        response = dialogpt_tokenizer.decode(\n",
        "            chat_history_ids[:, bot_input_ids.shape[-1]:][0],\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "        return response, chat_history_ids\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\", chat_history_ids\n",
        "\n",
        "\n",
        "\n",
        "# Test Monty with DialoGPT\n",
        "\n",
        "chat_history_ids = None  # reset conversation\n",
        "test_prompts = [\n",
        "    \"Hello Monty, how are you?\",\n",
        "    \"What’s your favorite color?\",\n",
        "    \"Tell me something interesting about space.\",\n",
        "    \"Can you give me advice on staying motivated?\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    reply, chat_history_ids = chat_with_monty(prompt, chat_history_ids)\n",
        "    print(\"=\"*50)\n",
        "    print(\"Input:\", prompt)\n",
        "    print(\"Monty:\", reply)\n"
      ],
      "metadata": {
        "id": "93Peig1bcHpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# STEP 6.1: FIXING LENGTH HANDLING\n",
        "\n",
        "\n",
        "def chat_with_monty(user_input, chat_history_ids=None):\n",
        "    try:\n",
        "        new_input_ids = dialogpt_tokenizer.encode(\n",
        "            user_input + dialogpt_tokenizer.eos_token,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        bot_input_ids = (\n",
        "            torch.cat([chat_history_ids, new_input_ids], dim=-1)\n",
        "            if chat_history_ids is not None else new_input_ids\n",
        "        )\n",
        "\n",
        "        # FIX: use max_new_tokens instead of max_length\n",
        "        chat_history_ids = dialogpt_model.generate(\n",
        "            bot_input_ids,\n",
        "            max_new_tokens=100,   # allows Monty to reply with up to 100 new tokens\n",
        "            pad_token_id=dialogpt_tokenizer.eos_token_id,\n",
        "            temperature=0.8,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        response = dialogpt_tokenizer.decode(\n",
        "            chat_history_ids[:, bot_input_ids.shape[-1]:][0],\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "        return response, chat_history_ids\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\", chat_history_ids\n",
        "\n",
        "\n",
        "\n",
        "      # Test Monty with DialoGPT\n",
        "\n",
        "chat_history_ids = None  # reset conversation\n",
        "test_prompts = [\n",
        "    \"Hello Monty, how are you?\",\n",
        "    \"What’s your favorite color?\",\n",
        "    \"Tell me something interesting about space.\",\n",
        "    \"Can you give me advice on staying motivated?\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    reply, chat_history_ids = chat_with_monty(prompt, chat_history_ids)\n",
        "    print(\"=\"*50)\n",
        "    print(\"Input:\", prompt)\n",
        "    print(\"Monty:\", reply)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2x__WJolc4tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y bitsandbytes\n",
        "!pip install -q bitsandbytes==0.43.1\n",
        "!pip install -q --upgrade transformers accelerate\n"
      ],
      "metadata": {
        "id": "urgAQD2-LI7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MONTY CHATBOT – FALCON-7B-INSTRUCT\n",
        "\n",
        "\n",
        "# Install / Upgrade required packages\n",
        "\n",
        "!pip install -q --upgrade transformers accelerate bitsandbytes\n",
        "!pip install -U bitsandbytes\n",
        "!pip install -U transformers accelerate\n",
        "\n",
        "# Import libraries\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "\n",
        "# Set device\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "\n",
        "# Model and tokenizer config\n",
        "\n",
        "MODEL_NAME = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "# Quantization config for a 4-bit loading\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "MAX_NEW_TOKENS = 150\n",
        "TEMPERATURE = 0.7\n",
        "TOP_K = 50\n",
        "TOP_P = 0.95\n",
        "REPETITION_PENALTY = 1.1\n",
        "MAX_INPUT_LENGTH = 1024\n",
        "\n",
        "#  Load tokenizer and model\n",
        "print(\"Loading Falcon-7B-Instruct in 4-bit mode...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Fix pad token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Critical fix: disable caching\n",
        "model.config.use_cache = False\n",
        "\n",
        "print(\"Monty loaded successfully on\", DEVICE)\n",
        "\n",
        "\n",
        "#  Chat function\n",
        "def chat_with_monty(user_input):\n",
        "    global chat_memory\n",
        "    prompt = build_prompt(user_input, chat_memory)\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=MAX_INPUT_LENGTH\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # disabled caching here explicitly\n",
        "    output_ids = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=MAX_NEW_TOKENS,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_k=TOP_K,\n",
        "        top_p=TOP_P,\n",
        "        repetition_penalty=REPETITION_PENALTY,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        use_cache=False    # <<---- important fix\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(\n",
        "        output_ids[0][inputs[\"input_ids\"].shape[-1]:],\n",
        "        skip_special_tokens=True\n",
        "    ).strip()\n",
        "\n",
        "    chat_memory.append({\"user\": user_input, \"monty\": response})\n",
        "    return response\n",
        "\n",
        "\n",
        "\n",
        "# Test Monty\n",
        "\n",
        "test_prompts = [\n",
        "    \"Hello Monty, how are you?\",\n",
        "    \"What’s your favorite color?\",\n",
        "    \"Tell me something interesting about space.\",\n",
        "    \"Can you give me advice on staying motivated?\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    reply = chat_with_monty(prompt)\n",
        "    print(\"=\" * 50)\n",
        "    print(\"User:\", prompt)\n",
        "    print(\"Monty:\", reply)\n"
      ],
      "metadata": {
        "id": "JmleU2M-iWQW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}